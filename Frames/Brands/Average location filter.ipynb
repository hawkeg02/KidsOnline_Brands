{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ca160f-b936-4aa6-8afc-94d993329e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from shapely.geometry import box\n",
    "import torchvision\n",
    "import torch\n",
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv('FINAL.csv')\n",
    "\n",
    "def parse_location(loc):\n",
    "    return np.array(list(map(float, loc.split(':'))))\n",
    "\n",
    "def timestamp_to_seconds(ts):\n",
    "    hours, minutes, seconds = ts.split('_')\n",
    "    total_seconds = int(hours) * 3600 + int(minutes) * 60 + float(seconds)\n",
    "    return total_seconds\n",
    "\n",
    "def find_second_to_last(arr, target):\n",
    "    indices = [i for i, value in enumerate(arr) if value == target]\n",
    "    if len(indices) >= 2:\n",
    "        return indices[-2]\n",
    "    return None\n",
    "\n",
    "def calculate_overlap(box1, box2):\n",
    "    x1_1, y1_1, x2_1, y2_1 = box1[0], box1[1], box1[0] + box1[2], box1[1] + box1[3]\n",
    "    x1_2, y1_2, x2_2, y2_2 = box2[0], box2[1], box2[0] + box2[2], box2[1] + box2[3]\n",
    "\n",
    "    x1_inter = max(x1_1, x1_2)\n",
    "    y1_inter = max(y1_1, y1_2)\n",
    "    x2_inter = min(x2_1, x2_2)\n",
    "    y2_inter = min(y2_1, y2_2)\n",
    "\n",
    "    inter_width = max(0, x2_inter - x1_inter)\n",
    "    inter_height = max(0, y2_inter - y1_inter)\n",
    "    overlap_area = inter_width * inter_height\n",
    "\n",
    "    return overlap_area\n",
    "\n",
    "def euclidean_distance_cxcywh(box1, box2):\n",
    "    cx1, cy1, _, _ = box1\n",
    "    cx2, cy2, _, _ = box2\n",
    "    distance = math.sqrt((cx1 - cx2) ** 2 + (cy1 - cy2) ** 2)\n",
    "    return distance\n",
    "\n",
    "df['timestamp_seconds'] = df['Timestamp'].apply(timestamp_to_seconds)\n",
    "df['location_coords'] = df['Location'].apply(parse_location)\n",
    "df = df.sort_values(by=['VideoFile', 'timestamp_seconds']).reset_index(drop=True)\n",
    "\n",
    "final_ids = {}\n",
    "unique_ids = {}\n",
    "temp_ids = {}\n",
    "result = []\n",
    "previous_time = None\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    pred = row['Prediction']\n",
    "    ts = row['timestamp_seconds']\n",
    "    loc = row['location_coords']\n",
    "    vf = row['VideoFile']\n",
    "    if ts != previous_time:\n",
    "        final_ids.update(temp_ids)\n",
    "        temp_ids={}\n",
    "    \n",
    "    # Predictions with the same name within 2 seconds\n",
    "    candidates = [\n",
    "    (uid, info) for uid, info in final_ids.items() \n",
    "        if info['Prediction'] == pred and abs(info['timestamp_seconds'] - ts) <= 2 and info['VideoFile'] == vf\n",
    "    ]\n",
    "    if candidates:\n",
    "        distances = [\n",
    "            (\n",
    "                uid, \n",
    "                np.linalg.norm(info['location_coords'] - loc) + 10 * (abs(info['timestamp_seconds'] - ts) // 0.25)\n",
    "            )\n",
    "            for uid, info in candidates\n",
    "        ]\n",
    "        closest_uid, closest_dist = min(distances, key=lambda x: x[1]) #Find the closest prediction\n",
    "        if closest_dist < 5000:  # Within a certain distance\n",
    "            row_numbers = [index for index, info in enumerate(unique_ids.values()) if info['timestamp_seconds'] == ts and info['unique_id'] == closest_uid] # is there another row with the same timestamp and prediction\n",
    "            if row_numbers:\n",
    "                timesb = list(unique_ids.values())[row_numbers[0]]['timestamp_seconds']\n",
    "                locb = list(unique_ids.values())[row_numbers[0]]['location_coords']\n",
    "                predb = list(unique_ids.values())[row_numbers[0]]['Prediction']\n",
    "                \n",
    "                indx = find_second_to_last(result, closest_uid)\n",
    "                if indx:\n",
    "                    tarlocx = df.loc[indx, 'location_coords']\n",
    "                    distance_a = euclidean_distance_cxcywh(loc, tarlocx)\n",
    "                    distance_b = euclidean_distance_cxcywh(locb, tarlocx)\n",
    "\n",
    "                    if distance_a < distance_b:\n",
    "                        unique_id = closest_uid\n",
    "                        locb_indx = len(result) - 1 - result[::-1].index(closest_uid)\n",
    "                        \n",
    "                        distances = [row for row in distances if row[0] != closest_uid] # find next best\n",
    "                        next_best = False\n",
    "                        if distances and len(distances) < 3:\n",
    "                            closest_uid, closest_dist = min(distances, key=lambda x: x[1])\n",
    "                            row_numbers = [index for index, info in enumerate(unique_ids.values()) if info['timestamp_seconds'] == ts and info['unique_id'] == closest_uid]\n",
    "                            next_best = True\n",
    "                        if next_best == True and not row_numbers:\n",
    "                            new_id = closest_uid\n",
    "                            result[locb_indx] = new_id\n",
    "                            unique_ids[new_id] = {'timestamp_seconds': timesb, 'location_coords': locb, 'Prediction': predb, 'unique_id': new_id, 'VideoFile' : vf}\n",
    "                            final_ids[new_id] = {'timestamp_seconds': timesb, 'location_coords': locb, 'Prediction': predb, 'unique_id': new_id, 'VideoFile' : vf}\n",
    "                        else:\n",
    "                            new_id = f\"{pred}_{len(set(result)) + 1:04d}\"\n",
    "                            result[locb_indx] = new_id\n",
    "                            unique_ids[new_id] = {'timestamp_seconds': timesb, 'location_coords': locb, 'Prediction': predb, 'unique_id': new_id, 'VideoFile' : vf}\n",
    "                            final_ids[new_id] = {'timestamp_seconds': timesb, 'location_coords': locb, 'Prediction': predb, 'unique_id': new_id, 'VideoFile' : vf}\n",
    "\n",
    "                    \n",
    "                    else:\n",
    "                        distances = [row for row in distances if row[0] != closest_uid] # find next best\n",
    "                        next_best = False\n",
    "                        if distances and len(distances) < 3:\n",
    "                            closest_uid, closest_dist = min(distances, key=lambda x: x[1])\n",
    "                            row_numbers = [index for index, info in enumerate(unique_ids.values()) if info['timestamp_seconds'] == ts and info['unique_id'] == closest_uid]\n",
    "                            next_best = True\n",
    "                        if next_best == True and not row_numbers:\n",
    "                            unique_id = closest_uid\n",
    "                        else:\n",
    "                            unique_id = f\"{pred}_{len(set(result)) + 1:04d}\" \n",
    "                else: # This means the previous occurance was the first so just make a new one\n",
    "                    unique_id = f\"{pred}_{len(set(result)) + 1:04d}\"\n",
    "            else: #This one means that there are no other rows in unique_ids with the same timestamp or unique_id\n",
    "                unique_id = closest_uid\n",
    "        else: # This one means that the distance is too far (unlikley)\n",
    "            unique_id = f\"{pred}_{len(set(result)) + 1:04d}\" \n",
    "    else: #There are no previous prediction with the same name and in the last 2 seconds\n",
    "        unique_id = f\"{pred}_{len(set(result)) + 1:04d}\"\n",
    "        \n",
    "    temp_ids[unique_id] = {'timestamp_seconds': ts, 'location_coords': loc, 'Prediction': pred, 'unique_id': unique_id, 'VideoFile' : vf}\n",
    "    unique_ids[unique_id] = {'timestamp_seconds': ts, 'location_coords': loc, 'Prediction': pred, 'unique_id': unique_id, 'VideoFile' : vf}\n",
    "    previous_time = ts\n",
    "    result.append(unique_id)\n",
    "\n",
    "df['unique_id'] = result\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%H_%M_%S.%f')\n",
    "df['Time'] = df['Timestamp'].dt.strftime('%H:%M:%S.%f').str[:-3]\n",
    "\n",
    "def average_location(group):\n",
    "    val = group.mean()\n",
    "    return val\n",
    "\n",
    "df[['x1', 'y1', 'x2', 'y2']] = pd.DataFrame(df['location_coords'].tolist(), index=df.index)\n",
    "\n",
    "result = df.groupby('unique_id').agg(\n",
    "    Student=('Student', 'first'),\n",
    "    Device=('Device', 'first'),\n",
    "    DateTime=('DateTime', 'first'),\n",
    "    VideoFile=('VideoFile', 'first'),\n",
    "    start_time=('Time', 'min'),\n",
    "    end_time=('Time', 'max'),\n",
    "    prediction=('Prediction', 'first'),\n",
    "    avg_confidence=('Confidence', lambda x: round(x.mean(), 2)),\n",
    "    frame_count=('Timestamp', 'count'),\n",
    "    avg_location_x1=('x1', 'median'),\n",
    "    avg_location_y1=('y1', 'median'),\n",
    "    avg_location_x2=('x2', 'median'),\n",
    "    avg_location_y2=('y2', 'median')\n",
    ").reset_index()\n",
    "\n",
    "result['avg_location'] = list(zip(result['avg_location_x1'], result['avg_location_y1'], result['avg_location_x2'], result['avg_location_y2']))\n",
    "result = result.drop(columns=['avg_location_x1', 'avg_location_y1', 'avg_location_x2', 'avg_location_y2'])\n",
    "result = result[result['frame_count'] > 1]\n",
    "result = result[result['avg_confidence'] > 0.71]\n",
    "result = result.drop(columns=['unique_id'])\n",
    "\n",
    "def filter(df):\n",
    "    df = df.sort_values(by=['VideoFile', 'start_time']).reset_index(drop=True)\n",
    "    VideoFile = df['VideoFile'].iloc[0]\n",
    "    End_Time = df['end_time'].iloc[0]\n",
    "    filtered_rows = []\n",
    "    temp_rows = []\n",
    "    New = True\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df['VideoFile'].iloc[i] != VideoFile:\n",
    "            VideoFile = df['VideoFile'].iloc[i]\n",
    "            End_Time = df['end_time'].iloc[i]\n",
    "            New = True\n",
    "        if temp_rows:\n",
    "            End_Time = max(df['end_time'].iloc[temp_rows])\n",
    "        if New or df['start_time'].iloc[i] > End_Time:\n",
    "            if temp_rows:\n",
    "                filtered_rows.extend(temp_rows)\n",
    "                temp_rows = []\n",
    "            End_Time = df['end_time'].iloc[i]\n",
    "            temp_rows.append(i)\n",
    "        else:\n",
    "            Add = True\n",
    "            for j in temp_rows:\n",
    "                if df['start_time'].iloc[i] < df['end_time'].iloc[j]:\n",
    "                    l_i = torchvision.ops.box_convert(torch.tensor(df['avg_location'].iloc[i]), in_fmt = \"cxcywh\", out_fmt = \"xywh\").numpy()\n",
    "                    l_j = torchvision.ops.box_convert(torch.tensor(df['avg_location'].iloc[j]), in_fmt = \"cxcywh\", out_fmt = \"xywh\").numpy()\n",
    "                    if calculate_overlap(l_i, l_j) > 100:\n",
    "                        base_i = df.loc[i, 'prediction'].split('-', 1)[0]\n",
    "                        base_j = df.loc[j, 'prediction'].split('-', 1)[0]\n",
    "                        if base_i == base_j:\n",
    "                            df.loc[j, 'start_time'] = min(df.loc[i, 'start_time'], df.loc[j, 'start_time'])\n",
    "                            df.loc[j, 'end_time'] = max(df.loc[i, 'end_time'], df.loc[j, 'end_time'])\n",
    "                            df.loc[j, 'avg_confidence'] = max(df.loc[i, 'avg_confidence'], df.loc[j, 'avg_confidence'])\n",
    "                            df.loc[j, 'frame_count'] = df.loc[i, 'frame_count'] + df.loc[j, 'frame_count']\n",
    "                            df.loc[j, 'prediction'] = base_i\n",
    "                            Add = False\n",
    "                        else:\n",
    "                            if df['avg_confidence'].iloc[i] > df['avg_confidence'].iloc[j]:\n",
    "                                temp_rows.remove(j)\n",
    "                                temp_rows.append(i)\n",
    "                                Add = False\n",
    "                            else:\n",
    "                                Add = False\n",
    "            if Add == True:\n",
    "                temp_rows.append(i)\n",
    "        New=False\n",
    "        if i == len(df)-1:\n",
    "            filtered_rows.extend(temp_rows)\n",
    "    return df.iloc[filtered_rows]\n",
    "\n",
    "def split_datetime(s):\n",
    "    parts = s.split(\"_\")\n",
    "    date_str = f\"{parts[0]} {parts[1]} {parts[2]}\".strip()\n",
    "    date = datetime.strptime(date_str, \"%b %d %Y\").strftime(\"%m/%d/%Y\")\n",
    "    time_str = f\"{parts[3][:2]}:{parts[3][2:]} {parts[4]}\"  # Time in 12-hour format (e.g., 1028 AM)\n",
    "    time_24hr = datetime.strptime(time_str, \"%I:%M %p\").strftime(\"%H:%M\")\n",
    "    return pd.Series([date, time_24hr])\n",
    "\n",
    "result = filter(result)\n",
    "result = result[~((result['frame_count'] < 4) & (result['avg_confidence'] <= 0.80))] \n",
    "result = result.sort_values(by=['VideoFile', 'start_time']).reset_index(drop=True)\n",
    "result[['Date', 'Time']] = result['DateTime'].apply(split_datetime)\n",
    "result = result.drop(columns=['avg_location', 'DateTime', 'VideoFile'])\n",
    "result.to_csv('final_processed_predictions_brands.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a3e02c-7bbd-4c3d-a415-82875acc38cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique rows saved to output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_unique_rows(file_path, output_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Drop duplicate rows based on 'Student', 'Divide', and 'DateTime'\n",
    "    unique_df = df.drop_duplicates(subset=['Student', 'Device', 'DateTime'])\n",
    "    \n",
    "    # Save the unique rows to a new CSV file\n",
    "    unique_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    return unique_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = \"FINAL.csv\"  # Replace with your actual file path\n",
    "    output_file = \"output.csv\"  # Replace with desired output file path\n",
    "    unique_rows = get_unique_rows(file_path, output_file)\n",
    "    print(f\"Unique rows saved to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d2a3bc-5229-4266-b976-019165d3c3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detr_env",
   "language": "python",
   "name": "detr_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
